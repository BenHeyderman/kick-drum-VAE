{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQP2AhQ8tlpq"
      },
      "source": [
        "### Drum sound VAE\n",
        "This notebook contains the implementation of a Variational Autoencoder (VAE) for generating drum sounds."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUfxrryvCcxn"
      },
      "source": [
        "## Project Set Up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3xViZYxO12y"
      },
      "outputs": [],
      "source": [
        "!pip install pytorch-ignite torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GQL1ET4MKH4H"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torchaudio\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import torchaudio.transforms as transforms\n",
        "import numpy as np\n",
        "import librosa\n",
        "from torchvision.utils import make_grid\n",
        "from IPython.display import Audio, display\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import soundfile as sf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3ZHL6JxTW00",
        "outputId": "2e453b28-c313-4970-c774-fcab18ee8bc1"
      },
      "outputs": [],
      "source": [
        "# Set manual seed to ensure reproducability\n",
        "SEED = 1221\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "# Get GPU if available\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMgndYxTBT-x"
      },
      "source": [
        "## Train Audio VAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3kQ16hFLnQp"
      },
      "outputs": [],
      "source": [
        "# Unzip kick samples\n",
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile(\"/content/4000Kicks.zip\") as archive:\n",
        "  archive.extractall(\"/content\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iR8vv60__kLn"
      },
      "outputs": [],
      "source": [
        "# Save log power spectrums to files and get normalisation scaler\n",
        "def compute_specs(path, new_SR, fft_length, hop_length):\n",
        "\n",
        "  # Get relevent audio paths\n",
        "  audio_file_paths = glob.glob(path)\n",
        "\n",
        "  # For temporarily storing magnitudes\n",
        "  magnitude_list = []\n",
        "\n",
        "  # Create hann window\n",
        "  hann_window = torch.hann_window(fft_length)\n",
        "\n",
        "  # For each audio file\n",
        "  for audio_file_path in audio_file_paths:\n",
        "      # Get the waveform\n",
        "      waveform, sample_rate = torchaudio.load(audio_file_path)\n",
        "\n",
        "      # Convert to mono\n",
        "      waveform = torch.mean(waveform, dim=0).unsqueeze(0)\n",
        "\n",
        "      # resample to new sample rate\n",
        "      transform = transforms.Resample(sample_rate, new_SR)\n",
        "      waveform = transform(waveform)\n",
        "\n",
        "      # Pad or truncate audio to be 1 second long\n",
        "      pad_width = new_SR - waveform.size(1)\n",
        "\n",
        "      if pad_width > 0:\n",
        "          waveform = torch.nn.functional.pad(waveform, (0, pad_width), 'constant', 0)\n",
        "      else:\n",
        "          waveform = waveform[:, :new_SR]\n",
        "\n",
        "          # Add fade if trancating\n",
        "          fade_length=int(new_SR/20)\n",
        "          fade_out = torch.linspace(1, 0, fade_length)\n",
        "          waveform[:, -fade_length:] *= fade_out\n",
        "\n",
        "      # Get STFT\n",
        "      stft = torch.stft(waveform, fft_length, hop_length, return_complex=True, window=hann_window)\n",
        "\n",
        "      # Take the magnitude of the complex numbers\n",
        "      magnitude = torch.abs(stft)\n",
        "\n",
        "      # Take the log of the magnitudes\n",
        "      log_magnitude = torch.log(magnitude + 1e-20)  # Adding a small constant to avoid log(0)\n",
        "\n",
        "      # Save log FFT tensor\n",
        "      magnitude_list.append(log_magnitude)\n",
        "\n",
        "  # Find min and max in log magnitude\n",
        "  all_magnitudes = torch.cat(magnitude_list, dim=0)\n",
        "  min_value = torch.min(all_magnitudes)\n",
        "  max_value = torch.max(all_magnitudes)\n",
        "\n",
        "  # Normalize using MinMaxScaler\n",
        "  scaler = MinMaxScaler()\n",
        "\n",
        "  # Fit the scaler with min and max values\n",
        "  scaler.fit([[min_value.item()], [max_value.item()]])\n",
        "  index = 0\n",
        "  for audio_file_path, log_magnitude in zip(audio_file_paths, magnitude_list):\n",
        "      fft_file_name = audio_file_path[:-3] + \"pt\"\n",
        "      # Normalize with MinMaxScaler\n",
        "      normalized_magnitude = torch.from_numpy(scaler.transform(log_magnitude.reshape(-1, 1)).reshape(log_magnitude.shape))\n",
        "      # Save as FFT file name\n",
        "      torch.save(normalized_magnitude, fft_file_name)\n",
        "      index += 1\n",
        "  return scaler\n",
        "\n",
        "\n",
        "new_SR = 44100\n",
        "fft_length = 512\n",
        "hop_length = 256\n",
        "\n",
        "# Run function for snares and kicks\n",
        "path = r'/content/4000Kicks/*.wav'\n",
        "kick_scaler = compute_specs(path, new_SR, fft_length, hop_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Kmw1J_8nJ5Nv"
      },
      "outputs": [],
      "source": [
        "# VAE dataset class\n",
        "class AudioDataset(Dataset):\n",
        "    def __init__(self, dir):\n",
        "        # Get locations of log specs\n",
        "        path = dir+\"*.pt\"\n",
        "        self.spec_file_paths = glob.glob(path)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.spec_file_paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Load in and return log spec\n",
        "        spec = torch.load(self.spec_file_paths[index]).to(torch.float32)\n",
        "        return spec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "gpF-R7WOO7zg",
        "outputId": "869c02c0-6a96-488b-dd2f-bb8f61813e4c"
      },
      "outputs": [],
      "source": [
        "def set_up_dataloaders(batch_size, data, train_split=0.8):\n",
        "\n",
        "    # train/test split\n",
        "    train_size = int(train_split * len(data))\n",
        "    test_size = len(data) - train_size\n",
        "\n",
        "    print(f\"Train size: {train_size}\")\n",
        "    print(f\"Test size: {test_size}\\n\")\n",
        "\n",
        "    train_dataset, test_dataset = torch.utils.data.random_split(data, [train_size, test_size])\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    return train_loader, val_loader\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "data = AudioDataset(\"/content/4000Kicks/\")\n",
        "kick_train_loader, kick_val_loader = set_up_dataloaders(batch_size, data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6B642heFoAwe"
      },
      "outputs": [],
      "source": [
        "class audio_VAE(nn.Module):\n",
        "    def __init__(self, num_channels=16, drop_out=0.2, latent_size=4):\n",
        "        super().__init__()\n",
        "        # Set base number of channels\n",
        "        self.num_channels = num_channels\n",
        "\n",
        "        # Output size list\n",
        "        self.desired_size = [None,None,None,None]\n",
        "\n",
        "        # Conv Layers\n",
        "        self.conv11 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1, out_channels=num_channels, kernel_size=(3,3), bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=num_channels, out_channels=num_channels, kernel_size=(3,3), bias=False),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.conv12 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=num_channels, out_channels=num_channels, kernel_size=(3,3), bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=num_channels, out_channels=num_channels, kernel_size=(3,3), bias=False),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.conv13 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=num_channels, out_channels=num_channels*2, kernel_size=(3,3), bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=num_channels*2, out_channels=num_channels*2, kernel_size=(3,3), bias=False),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.conv14 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=num_channels*2, out_channels=num_channels*2, kernel_size=(3,3), bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=num_channels*2, out_channels=num_channels*2, kernel_size=(3,3), bias=False),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.conv15 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=num_channels*2, out_channels=num_channels*2, kernel_size=(3,3), bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=num_channels*2, out_channels=num_channels*2, kernel_size=(3,3), bias=False),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "\n",
        "        # Flatten\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        # Encoder fully connected layers\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Linear(8*3*num_channels*2, latent_size)\n",
        "        )\n",
        "        self.fc2 = nn.Sequential(\n",
        "            nn.Linear(8*3*num_channels*2, latent_size)\n",
        "        )\n",
        "\n",
        "        # Decoder fully connected layer\n",
        "        self.fc3 = nn.Sequential(\n",
        "            nn.Linear(latent_size, 8*3*num_channels*2)\n",
        "        )\n",
        "\n",
        "        # Deconv Layers\n",
        "        self.conv21 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels=num_channels*2, out_channels=num_channels*2, kernel_size=(3,3), bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(in_channels=num_channels*2, out_channels=num_channels*2, kernel_size=(3,3), bias=False),\n",
        "            nn.ReLU(),\n",
        "            )\n",
        "        self.conv22 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels=num_channels*2, out_channels=num_channels*2, kernel_size=(3,3), bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(in_channels=num_channels*2, out_channels=num_channels*2, kernel_size=(3,3), bias=False),\n",
        "            nn.ReLU(),\n",
        "            )\n",
        "        self.conv23 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels=num_channels*2, out_channels=num_channels*2, kernel_size=(3,3), bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(in_channels=num_channels*2, out_channels=num_channels, kernel_size=(3,3), bias=False),\n",
        "            nn.ReLU(),\n",
        "            )\n",
        "        self.conv24 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels=num_channels, out_channels=num_channels, kernel_size=(3,3), bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(in_channels=num_channels, out_channels=num_channels, kernel_size=(3,3), bias=False),\n",
        "            nn.ReLU(),\n",
        "            )\n",
        "        self.conv25 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels=num_channels, out_channels=num_channels, kernel_size=(3,3), bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(in_channels=num_channels, out_channels=1, kernel_size=(3,3), bias=False),\n",
        "            nn.ReLU(),\n",
        "            # Set output between 0 and 1\n",
        "            nn.Sigmoid()\n",
        "            )\n",
        "\n",
        "        # Initialise weights\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    # Encoder function\n",
        "    # Pass through convolution and pooling layers\n",
        "    # Save desired size before each pooling for decoder\n",
        "    # Flatten output\n",
        "    def encode(self, x):\n",
        "\n",
        "      x = self.conv11(x)\n",
        "      self.desired_size[0] = [x.shape[2], x.shape[3]]\n",
        "      x = F.max_pool2d(x, kernel_size=(2, 2), padding=0)\n",
        "      x = self.conv12(x)\n",
        "      self.desired_size[1] = [x.shape[2], x.shape[3]]\n",
        "      x = F.max_pool2d(x, kernel_size=(2, 2), padding=0)\n",
        "      x = self.conv13(x)\n",
        "      self.desired_size[2] = [x.shape[2], x.shape[3]]\n",
        "      x = F.max_pool2d(x, kernel_size=(2, 2), padding=0)\n",
        "      x = self.conv14(x)\n",
        "      self.desired_size[3] = [x.shape[2], x.shape[3]]\n",
        "      x = F.max_pool2d(x, kernel_size=(2, 2), padding=0)\n",
        "      x = self.conv15(x)\n",
        "\n",
        "      x = self.flatten(x)\n",
        "      return x\n",
        "\n",
        "    # Decoder function\n",
        "    # Fully connected layer\n",
        "    # Unflatten\n",
        "    # Pass through convolutional layers and upsample\n",
        "    def decode(self, x):\n",
        "        x = self.fc3(x)\n",
        "        x = x.view(-1, self.num_channels*2, 8, 3)\n",
        "        x = self.conv21(x)\n",
        "        x = nn.Upsample(size=self.desired_size[3], mode='bilinear')(x)\n",
        "        x = self.conv22(x)\n",
        "        x = nn.Upsample(size=self.desired_size[2], mode='bilinear')(x)\n",
        "        x = self.conv23(x)\n",
        "        x = nn.Upsample(size=self.desired_size[1], mode='bilinear')(x)\n",
        "        x = self.conv24(x)\n",
        "        x = nn.Upsample(size=self.desired_size[0], mode='bilinear')(x)\n",
        "        x = self.conv25(x)\n",
        "        return x\n",
        "\n",
        "    # Init weights\n",
        "    # Function from Queen Mary Deep Learning for Audio and Music module\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
        "            torch.nn.init.kaiming_uniform_(module.weight)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.BatchNorm2d):\n",
        "            torch.nn.init.ones_(module.weight)\n",
        "\n",
        "    # Convert latent space to log mag\n",
        "    def generate(self, z):\n",
        "      reconstructed = self.decode(z)\n",
        "      return reconstructed\n",
        "\n",
        "    # Convert mu and logvar to latent representation\n",
        "    def reparam(self, mu, logvar):\n",
        "      std = torch.exp(0.5*logvar)\n",
        "      eps = torch.randn_like(std)\n",
        "      z = mu + eps*std\n",
        "      return z\n",
        "\n",
        "    # Get Latent representation from input audio\n",
        "    def get_latent(self, x):\n",
        "      x = self.encode(x)\n",
        "      mu = self.fc1(x)\n",
        "      logvar = self.fc2(x)\n",
        "      z = self.reparam(mu, logvar)\n",
        "      return z\n",
        "\n",
        "    # Foward pass\n",
        "    def forward(self, x):\n",
        "        # Encode\n",
        "        x = self.encode(x)\n",
        "\n",
        "        # Get mu and logvar\n",
        "        mu = self.fc1(x)\n",
        "        logvar = self.fc2(x)\n",
        "\n",
        "        # Get latent representation\n",
        "        z = self.reparam(mu, logvar)\n",
        "\n",
        "        # Decode\n",
        "        reconstructed = self.decode(z)\n",
        "\n",
        "        return reconstructed, mu, logvar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhf72wrETMRm"
      },
      "outputs": [],
      "source": [
        "# Set up kick model and optimizer\n",
        "\n",
        "kick_model = audio_VAE().to(device)\n",
        "\n",
        "kick_optimizer = optim.Adam(kick_model.parameters(), lr=1e-3)\n",
        "\n",
        "# Loss Function\n",
        "\n",
        "bce_loss = nn.BCELoss(reduction='sum')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNralkt4HGbx"
      },
      "outputs": [],
      "source": [
        "# *** NOT ORIGINAL CODE ***\n",
        "# Original from https://arxiv.org/abs/1312.6114\n",
        "# Code from https://medium.com/@judyyes10/generate-images-using-variational-autoencoder-vae-4d429d9bdb5\n",
        "# Kullback-Leibler divergence\n",
        "def kld_loss(x_pred, x, mu, logvar):\n",
        "    return -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guYzTKlR1TKl",
        "outputId": "9691ebf6-00a7-4573-bc14-19b930149a4c"
      },
      "outputs": [],
      "source": [
        "# Evaluate model Performance\n",
        "def evaluate(model, data_loader, KLD_multiplier):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_KLD = 0\n",
        "\n",
        "    # Disable gradient calculation during evaluation\n",
        "    with torch.no_grad():\n",
        "        for x in data_loader:\n",
        "            # Input to device\n",
        "            x = x.to(device)\n",
        "            # Get outputs\n",
        "            x_pred, mu, logvar = model(x)\n",
        "            BCE = bce_loss(x_pred, x)\n",
        "            KLD = kld_loss(x_pred, x, mu, logvar)\n",
        "            loss = BCE + KLD_multiplier*KLD\n",
        "            # Sum loss\n",
        "            total_loss += loss\n",
        "            total_KLD += KLD\n",
        "\n",
        "    # Calculate epoch loss metrics\n",
        "\n",
        "    epoch_loss = total_loss / len(data_loader)\n",
        "    epoch_KLD = total_KLD / len(data_loader)\n",
        "\n",
        "    return epoch_loss, epoch_KLD\n",
        "\n",
        "# Train Model\n",
        "def train(model, train_loader, valid_loader, optimizer, num_epochs, saved_model, evaluate_every_n_epochs=1, KLD_multiplier=100):\n",
        "\n",
        "    # For storing losses\n",
        "    train_losses = []\n",
        "    valid_losses = []\n",
        "    train_KLD_losses = []\n",
        "    valid_KLD_losses = []\n",
        "\n",
        "    # Init loss as very high number\n",
        "    best_valid_loss = 99999999999999999\n",
        "\n",
        "    # For each epoch\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        epoch_KLD = 0\n",
        "        # For each batch in data loader\n",
        "        for x in train_loader:\n",
        "            # Input to device\n",
        "            x = x.to(device)\n",
        "\n",
        "            # Get Output\n",
        "            x_pred, mu, logvar = model(x)\n",
        "            # Calculate losses\n",
        "            BCE = bce_loss(x_pred, x)\n",
        "            KLD = kld_loss(x_pred, x, mu, logvar)\n",
        "            loss = BCE + KLD_multiplier*KLD\n",
        "\n",
        "            # Back Prop\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss\n",
        "            epoch_KLD += KLD\n",
        "        # Epoch metrics\n",
        "        epoch_loss /= len(train_loader)\n",
        "        epoch_KLD /= len(train_loader)\n",
        "\n",
        "        # Save/print epoch metrics\n",
        "        train_losses.append(epoch_loss.detach().cpu())\n",
        "        train_KLD_losses.append(epoch_KLD.detach().cpu())\n",
        "        print(f'Epoch:{epoch+1}, Training Loss:{epoch_loss:.4f}')\n",
        "        print(f'Training KLD:{epoch_KLD:.4f}')\n",
        "        # Evaluate the network on the validation data\n",
        "        if((epoch+1) % evaluate_every_n_epochs == 0):\n",
        "            valid_loss, valid_KLD = evaluate(model, valid_loader, KLD_multiplier)\n",
        "\n",
        "            print(f'Validation loss: {valid_loss:.6f}')\n",
        "            print()\n",
        "            valid_losses.append(valid_loss.detach().cpu())\n",
        "            valid_KLD_losses.append(valid_KLD.detach().cpu())\n",
        "\n",
        "            # If model is best model for validation set, save model\n",
        "            if valid_loss<best_valid_loss:\n",
        "              print(\"New best model, saving...\\n\")\n",
        "              best_valid_loss = valid_loss\n",
        "              torch.save(model.state_dict(), \"best_model.pkl\")\n",
        "\n",
        "    return train_losses, valid_losses, train_KLD_losses, valid_KLD_losses, model\n",
        "\n",
        "num_epochs = 20\n",
        "\n",
        "# Train\n",
        "train_losses, valid_losses, train_KLD_losses, valid_KLD_losses, snare_model = train(kick_model, kick_train_loader, kick_val_loader, kick_optimizer, num_epochs, 'best_model.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "thoxo1i3LnQv",
        "outputId": "61f04a1a-a397-47ca-a242-da66cdd25a38"
      },
      "outputs": [],
      "source": [
        "# Plot loss\n",
        "plt.plot(range(num_epochs), train_losses, 'dodgerblue', label='training')\n",
        "plt.plot(range(num_epochs), valid_losses, 'orange', label='validation')\n",
        "plt.xlim(0, num_epochs);\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss on Training/Validation Set')\n",
        "plt.legend();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "d5q04svFLnQv",
        "outputId": "571d8f39-1a62-43fb-a3b6-556b656209cc"
      },
      "outputs": [],
      "source": [
        "# plot KLD loss\n",
        "plt.plot(range(num_epochs), train_KLD_losses, 'dodgerblue', label='training')\n",
        "plt.plot(range(num_epochs), valid_KLD_losses, 'orange', label='validation')\n",
        "plt.xlim(0, num_epochs);\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('KLD')\n",
        "plt.title('KL Divergence on Training/Validation Set')\n",
        "plt.legend();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "m1CxT1PiFFlD",
        "outputId": "436eb104-bcda-4173-a868-032cd6f338d3"
      },
      "outputs": [],
      "source": [
        "# *** ADAPTED FROM https://medium.com/@judyyes10/generate-images-using-variational-autoencoder-vae-4d429d9bdb5\n",
        "\n",
        "# Visualise input vs output\n",
        "def compare_images(model, sample_images):\n",
        "    reconstructed_images = model(sample_images)[0]\n",
        "    comparison = torch.cat([sample_images, reconstructed_images])\n",
        "    comparison_image = make_grid(comparison.detach().cpu(), nrow=8)\n",
        "    fig = plt.figure(figsize=(5, 5))\n",
        "    output = plt.imshow(comparison_image.permute(1, 2, 0))\n",
        "    plt.show()\n",
        "\n",
        "for batch in kick_train_loader:\n",
        "    x= batch\n",
        "    break\n",
        "sample_images = x.to(device)\n",
        "\n",
        "compare_images(snare_model, sample_images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2q4WsiXA19J"
      },
      "source": [
        "## Save/Load Audio Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Tqve1cM1AsLa"
      },
      "outputs": [],
      "source": [
        "# Directories of kick/snare models\n",
        "kick_model_dir = \"/content/Kick_VAE_Model_4.pkl\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "leQ_n6U0_0Zt"
      },
      "outputs": [],
      "source": [
        "# Save model\n",
        "#torch.save(snare_model.state_dict(), kick_model_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b52A3UbxACJl",
        "outputId": "af03381e-fa7b-4e9a-ec26-fd058afcabc6"
      },
      "outputs": [],
      "source": [
        "#Load models\n",
        "\n",
        "kick_model = audio_VAE()\n",
        "kick_model.load_state_dict(torch.load(kick_model_dir))\n",
        "\n",
        "kick_model = kick_model.to(device)\n",
        "\n",
        "kick_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9K6CEEWCknS2"
      },
      "outputs": [],
      "source": [
        "# Normalise latent space range for taining set\n",
        "def get_latent_space_scale(model, data_loader, latent_size):\n",
        "\n",
        "  highest_max = torch.zeros(latent_size)\n",
        "  lowest_min = torch.zeros(latent_size)\n",
        "\n",
        "  with torch.no_grad():\n",
        "        # For each pattern in data loader\n",
        "        for pattern in data_loader:\n",
        "          pattern = pattern.to(device)\n",
        "          latent_space = model.get_latent(pattern)\n",
        "          # Get max and min\n",
        "          max = torch.max(latent_space, dim=0).values #check\n",
        "          min = torch.min(latent_space, dim=0).values #check\n",
        "\n",
        "          # Update global max/min\n",
        "          for i in range(latent_size):\n",
        "            if min[i] < lowest_min[i]:\n",
        "              lowest_min[i] = min[i]\n",
        "            if max[i] > highest_max[i]:\n",
        "              highest_max[i] = max[i]\n",
        "\n",
        "  return [lowest_min.tolist(), highest_max.tolist()]\n",
        "\n",
        "# Seperate scaler for kick and snare\n",
        "kick_boundaries = get_latent_space_scale(kick_model, kick_train_loader, 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_JqCIPl2tWFJ"
      },
      "outputs": [],
      "source": [
        "# Apply boundaries to scaler\n",
        "\n",
        "kick_latent_scaler = MinMaxScaler()\n",
        "kick_latent_scaler.fit(kick_boundaries)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MuvRE2ts0ZEN"
      },
      "outputs": [],
      "source": [
        "# Convert prediction to waveform\n",
        "def pred2wave(x_pred, scaler, n_iter=100):\n",
        "  # move prediction to cpu, remove gradient, remove dimension\n",
        "  x_pred = x_pred.squeeze(0).detach().cpu()\n",
        "\n",
        "  # Invert amp normalisation\n",
        "  inverse_normalized_magnitude = torch.from_numpy(scaler.inverse_transform(x_pred.view(-1, 1)).reshape(x_pred.shape))\n",
        "\n",
        "  # Invert log amp\n",
        "  mag = torch.exp(inverse_normalized_magnitude).numpy()\n",
        "\n",
        "  # Griffin Lim to estimate phase\n",
        "  waveform = librosa.griffinlim(mag, n_iter=n_iter, window='hann', hop_length=hop_length)\n",
        "  return waveform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ereA24ADip6x"
      },
      "source": [
        "## UI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "OCfxMU2ShLPe",
        "outputId": "906ce884-c160-485b-d1fa-81fb575b832c"
      },
      "outputs": [],
      "source": [
        "#@title Kick Generator\n",
        "sr = 44100\n",
        "#@markdown Latent Space:\\\n",
        "#@markdown _0 and 1 are the minimum and maximum values from the training set._\n",
        "#@markdown _Values outside of this range are out of distribution._\n",
        "\n",
        "dim_1 = 0.59 #@param {type:'slider', min:-0.5, max:1.5, step:0.01}\n",
        "dim_2 = 0.12 #@param {type:'slider', min:-0.5, max:1.5, step:0.01}\n",
        "dim_3 = 0.28 #@param {type:'slider', min:-0.5, max:1.5, step:0.01}\n",
        "dim_4 = 0.3 #@param {type:'slider', min:-0.5, max:1.5, step:0.01}\n",
        "#@markdown Filename:\\\n",
        "#@markdown _Do not include extension._\n",
        "file_name = \"kick\" # @param {type:\"string\"}\n",
        "\n",
        "kick_path = \"/content/DrumSound/\"+file_name+\".wav\"\n",
        "\n",
        "# Denormalise\n",
        "latent_space = torch.FloatTensor(kick_latent_scaler.inverse_transform([[dim_1, dim_2, dim_3, dim_4]]))\n",
        "# Get output\n",
        "x_pred = kick_model.generate(latent_space.to(device))\n",
        "\n",
        "# Convert to Audio\n",
        "waveform = pred2wave(x_pred, kick_scaler)\n",
        "display(Audio(waveform, rate=sr))\n",
        "waveform = librosa.util.normalize(np.ravel(waveform))\n",
        "sf.write(kick_path, waveform, sr, subtype='PCM_24')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "DUfxrryvCcxn",
        "QMgndYxTBT-x",
        "J2q4WsiXA19J",
        "9A3VBgzdynzJ",
        "spcrU0_LtC-C",
        "RjXHlWJr3gQK",
        "kNu6bs56IvAW"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
